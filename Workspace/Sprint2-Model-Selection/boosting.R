# Read clean data
library(readr)     # for fast reading of input files
library(here)
library(bestglm)
library(caret)
library(pROC)

source("./util_funcs.R")

# >>> Load the dataset <<< #
diabetic_data0 <- 
  here("Datasets", "diabetic_retinopathyDataSet_train.csv") %>% 
  read.csv(header = TRUE, stringsAsFactors = F)

# >>> Assign feature names <<< #
names(diabetic_data0) <-  
  c("quality", "pre_screening", 
    "ma1", "ma2", "ma3", "ma4", "ma5", "ma6", 
    "exudate1", "exudate2", "exudate3", "exudate4",
    "exudate5", "exudate6", "exudate7","exudate8",
    "macula_opticdisc_distance", "opticdisc_diameter",
    "am_fm_classification", "classes")

# >>> Recode numeric outcome variable to string, then factor <<< #
diabetic_data1 <- diabetic_data0 %>%
  mutate(classes = ifelse(classes == 0, "No",
                          ifelse(classes == 1, "Sign", NA))) %>%
  mutate(classes = as_factor(classes))

# >>> Generate Table of Feature Summary by Class <<< #
arsenal::tableby(classes~., data = diabetic_data1, total= TRUE) %>% 
  summary(text = TRUE)
# Build custom AUC function to extract AUC
# from the caret model object
eval_mod <- function(model, data) {
  pred <- predict(model, data)
  cm <- caret::confusionMatrix(pred, data$classes, positive="malignant")
  auc <- roc(data$classes,
             predict(model, data, type = "prob")[, "malignant"]) %>% auc()
  result <- c(cm$overall["Accuracy"],cm$byClass['Sensitivity'], cm$byClass['Specificity'], cm$byClass['F1'],AUC=auc)
  return(result)
}

bc_data0 <-  read.csv(paste0("http://archive.ics.uci.edu/ml/machine-learning-databases/","breast-cancer-wisconsin/breast-cancer-wisconsin.data"), header = FALSE, stringsAsFactors = F)

names (bc_data0) <-  c("sample_code_number", 
                       "clump_thickness", 
                       "uniformity_of_cell_size", 
                       "uniformity_of_cell_shape", 
                       "marginal_adhesion", 
                       "single_epithelial_cell_size", 
                       "bare_nuclei", 
                       "bland_chromatin", 
                       "normal_nucleoli", 
                       "mitosis", 
                       "classes")

str(bc_data0)
bc_data0$bare_nuclei = as.integer(bc_data0$bare_nuclei)
bc_data1 <- bc_data0 %>%
  dplyr::mutate(classes = ifelse(classes == "2", "benign",
                                 ifelse(classes == "4", "malignant", NA)))

# De-duplicate observations----
bc_data2 <- bc_data1 %>% distinct(sample_code_number,.keep_all = TRUE)
row.names(bc_data2) <- bc_data2$sample_code_number
bc_data3 <- bc_data2 %>% dplyr::select(-sample_code_number)
bc_data3 <- na.omit(bc_data3)
bc_data <- bc_data3

bc_data$classes <- as.factor(bc_data$classes)

set.seed(2024)
index <- caret::createDataPartition(bc_data$classes, p = 0.7, list = FALSE)

train_data <- bc_data[index, ]
test_data  <- bc_data[-index, ]

train_data$classes %>% table(.)
set.seed(2024)

ctrl <- trainControl(method = "repeatedcv", 
                     number = 5, 
                     repeats = 3,  
                     savePredictions = TRUE,
                     summaryFunction = twoClassSummary,
                     classProbs = TRUE)
set.seed(2024)

forest_fit <- train(
  classes ~ .,
  data = train_data,                         
  method = "ranger",
  metric = "ROC",
  trControl = trainControl(method = "cv", number = 5, classProbs = TRUE, summaryFunction = twoClassSummary),
  importance="impurity"
)
forest <- eval_mod(forest_fit,test_data)

set.seed(2024)

gbm_fit <- train(classes ~ .,
                 data = train_data,
                 method = "gbm",
                 verbose = FALSE,
                 metric = "Sens",
                 trControl = ctrl)

gbm <- eval_mod(gbm_fit,test_data)

modelLookup("xgbTree")

# nrounds: number of boosting iterations, M
# max_depth: maximum tree depth
# eta: shrinkage,  Î·
# gamma: minimum loss reduction
# colsamle_bytree: subsample ratio of columns
# min_child_weight: minimum size of instance weight
# substample: subsample percentage
# An alternative to tuneGrid
# tuneLength argument is used to control the number of combinations generated by this random tuning parameter search.
# 
# 
set.seed(2024)

xgboostT_fit <- train(classes ~ .,
                      data = train_data,
                      method = "xgbTree",
                      verbose = FALSE,
                      metric = "Sens",
                      trControl = ctrl)


xgboost_tree <- eval_mod(xgboostT_fit,test_data)


set.seed(2024)

xgboostL_fit <- train(classes ~ .,
                      data = train_data,
                      method = "xgbLinear",
                      verbose = FALSE,
                      tuneLength=5,
                      metric = "Sens",
                      trControl = ctrl)

xgboost_linear <- eval_mod(xgboostL_fit,test_data)


rbind(forest, gbm, xgboost_tree, xgboost_linear)


# Variable importance
# 
# Calculate variable importance using vip
# 
varImp(xgboostL_fit)

vip::vip(xgboostL_fit , num_features = 10) 

vip::vip(xgboostT_fit , num_features = 10) 
vip::vip(forest_fit , num_features = 10) 
